{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing"
      ],
      "metadata": {
        "id": "mBVnkzECbl7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import library yang dibutuhkan"
      ],
      "metadata": {
        "id": "VHciMeyHboGa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4XAbgRZ3CAQ"
      },
      "outputs": [],
      "source": [
        "print(\"Detecting environment: \", end=' ')\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running the code in Google Colab. Installing and downloading dependencies.\\nPlease wait...\")\n",
        "    import nltk\n",
        "    !wget https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/lib/taudataNlpTm.py\n",
        "    !mkdir data\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/slang.txt\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_id.txt\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_en.txt\n",
        "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/corpus_sederhana.txt\n",
        "    !pip install unidecode textblob sastrawi\n",
        "    nltk.download('popular')\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running the code locally.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from unidecode import unidecode\n",
        "from html import unescape\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "nltk.download('popular')"
      ],
      "metadata": {
        "id": "rdEsfBqK3Ovu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load stopwords"
      ],
      "metadata": {
        "id": "q8N9wSACbuDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Stopwords: Ada beberapa cara\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "factory = StopWordRemoverFactory()\n",
        "\n",
        "NLTK_StopWords = stopwords.words('indonesian')\n",
        "Sastrawi_StopWords_id = factory.get_stop_words()\n",
        "\n",
        "df=open('data/stopwords_id.txt',\"r\",encoding=\"utf-8\", errors='replace')\n",
        "id_stop = df.readlines()\n",
        "df.close()\n",
        "id_stop = [t.strip().lower() for t in id_stop]\n",
        "\n",
        "with open('more_stopwords.txt', 'r') as f:\n",
        "    kamus = f.readline().split(',')\n",
        "    more_stopwords = []\n",
        "    for x in kamus:\n",
        "        text = re.sub('[\\s]+', '', x)\n",
        "        more_stopwords.append(text)\n",
        "\n",
        "with open('exclude_stopwords.txt', 'r') as f:\n",
        "    kamus = f.readline().split(',')\n",
        "    exclude_stopwords = []\n",
        "    for x in kamus:\n",
        "        text = re.sub('[\\s]+', '', x)\n",
        "        exclude_stopwords.append(text)\n",
        "\n",
        "N = 10\n",
        "print(NLTK_StopWords[:N])\n",
        "print(Sastrawi_StopWords_id[:N])\n",
        "print(id_stop[:N])\n",
        "#print(more_stopwords[:N])\n",
        "print(len(Sastrawi_StopWords_id), len(id_stop), len(NLTK_StopWords), len(more_stopwords))\n",
        "print(type(Sastrawi_StopWords_id), type(id_stop), type(NLTK_StopWords), type(more_stopwords))\n",
        "\n",
        "#Stopword\n",
        "stops = set(Sastrawi_StopWords_id + id_stop + NLTK_StopWords + more_stopwords)\n",
        "#exclude = [\"tidak\", 'benar', 'betul', 'baik', 'belum', 'boleh', 'dekat', 'guna', 'mampu', 'masalah', 'nggak', 'pasti', 'penting', 'percuma', 'rasa', 'satu', 'tegas', 'tunjuk', 'yakin', 'usah']\n",
        "exclude = exclude_stopwords\n",
        "for i in exclude:\n",
        "  stops.discard(i)\n",
        "print(len(stops))"
      ],
      "metadata": {
        "id": "QtxTN9Bg3UY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load slang words"
      ],
      "metadata": {
        "id": "qIyO_mH6bxj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sumber github\n",
        "indo_slang_words = pd.read_csv(\"https://raw.githubusercontent.com/nasalsabila/kamus-alay/master/colloquial-indonesian-lexicon.csv\")\n",
        "indo_slang_words = indo_slang_words.iloc[:,:2]\n",
        "print('jumlah slang words: ', len(indo_slang_words))\n",
        "# sumber tau data\n",
        "df=open('data/slang.txt',\"r\",encoding=\"utf-8\", errors='replace')\n",
        "slangS = df.readlines(); df.close()\n",
        "slangS = [t.strip('\\n').strip() for t in slangS] # remove enter\n",
        "slangS = [t.split(\":\") for t in slangS] # split based on ':'\n",
        "slangS = [[k.strip(), v.strip()] for k,v in slangS] # remove white space\n",
        "slangS = np.array(slangS) # convert to numpy\n",
        "slang = slangS[:,0]\n",
        "formal = slangS[:,1]\n",
        "more_slang_words = pd.DataFrame(slangS[:,0], columns=['slang'])\n",
        "more_slang_words['formal'] = formal\n",
        "print('jumlah slang words: ', len(more_slang_words))\n",
        "# combine slang words\n",
        "all_slang_words = pd.concat([indo_slang_words, more_slang_words])\n",
        "all_slang_words.drop_duplicates(subset='slang', keep='first', inplace=True)\n",
        "print('jumlah slang words: ', len(all_slang_words))\n",
        "slang_words = all_slang_words.set_index('slang').T.to_dict(orient='records')\n",
        "dict_slang_words = slang_words[0]\n",
        "print('kata normal dari kata nggak yaitu: ', dict_slang_words['ga'])"
      ],
      "metadata": {
        "id": "Dvr0CtNf3Ykm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fungsi text preprocessing"
      ],
      "metadata": {
        "id": "l9IsngaSb0Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Emoji\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "def hashtags(text):\n",
        "  \"\"\"Menyimpan hashtags\"\"\"\n",
        "  getHashtags = re.compile(r\"#(\\w+)\")\n",
        "  pisahtags = re.compile(r'[\\w][^A-Z]*')\n",
        "  tagS = re.findall(getHashtags, text)\n",
        "  for tag in tagS:\n",
        "      proper_words = ' '.join(re.findall(pisahtags, tag))\n",
        "      text = text.replace('#'+tag,proper_words)\n",
        "  return text\n",
        "\n",
        "def removeConsecutiveDuplicates(text):\n",
        "    if len(text) < 2:\n",
        "        return text\n",
        "    if text[0] != text[1]:\n",
        "        return text[0]+removeConsecutiveDuplicates(text[1:])\n",
        "    return removeConsecutiveDuplicates(text[1:])\n",
        "\n",
        "def normalization(text):\n",
        "  T = TextBlob(text).words\n",
        "  for i,t in enumerate(T):\n",
        "      if t in dict_slang_words.keys():\n",
        "          T[i] = dict_slang_words[t]\n",
        "  return ' '.join(T)\n",
        "\n",
        "def datePreproc(date):\n",
        "    date = re.sub('(T[\\w:+]*)', '', str(date))\n",
        "    return date\n",
        "\n",
        "urlPattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "\n",
        "def caseFolding(text):\n",
        "    #Convert to lower case\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def cleansing1(text):\n",
        "    #Get hashtags\n",
        "    #text = hashtags(text)\n",
        "    #Remove hashtags\n",
        "    text = re.sub('#(\\w+)', ' ', text)\n",
        "    #Remove enter\n",
        "    text = re.sub('(\\n)', ' ', text)\n",
        "    #Representasi ASCII terdekat\n",
        "    text = unidecode(text)\n",
        "    #Clean html entity\n",
        "    text = unescape(text)\n",
        "    #Remove emoji\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    #Remove email\n",
        "    text = re.sub('[\\w._%+-]+@[\\w\\.-]+\\.[a-zA-Z]{2,4}', ' ', text)\n",
        "    #Remove @username\n",
        "    text = re.sub('@[\\w]*', ' ', text)\n",
        "    #Remove Website URLS\n",
        "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)\n",
        "    #Remove additional white spaces\n",
        "    text = re.sub('[\\s]+', ' ', text)\n",
        "    return text\n",
        "\n",
        "def cleansing2(text):\n",
        "    #Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]|_', ' ', text)\n",
        "    #Remove number\n",
        "    text = re.sub('[\\d]+', ' ', text)\n",
        "    #Remove double character\n",
        "    text = removeConsecutiveDuplicates(text) #https://www.geeksforgeeks.org/remove-consecutive-duplicates-string/\n",
        "    #Remove additional white spaces\n",
        "    text = re.sub('[\\s]+', ' ', text)\n",
        "    return text\n",
        "\n",
        "def stopwordRemover(word_tokens):\n",
        "    '''input berupa kalimat yang sudah ditoken (from nltk.tokenize import word_tokenize)'''\n",
        "    word_no_stopwords = [w for w in word_tokens if not w in stops]\n",
        "    word_no_stopwords = \" \".join(word_no_stopwords)\n",
        "    return word_no_stopwords\n",
        "\n",
        "def preprocess(text):\n",
        "    stemmer = StemmerFactory().create_stemmer()\n",
        "    text = str(text)\n",
        "    text = cleansing1(text) \n",
        "    lower_text = caseFolding(text) \n",
        "    formal_text = normalization(lower_text) \n",
        "    clean_text = cleansing2(formal_text) \n",
        "    words = word_tokenize(clean_text) \n",
        "    #text = \" \".join(words)\n",
        "    text = stopwordRemover(words)\n",
        "    text = stemmer.stem(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "AVEOBqXy3bDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset"
      ],
      "metadata": {
        "id": "7iopwneWb5wM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_kandidat = pd.read_csv('/content/new_prabowo.csv', on_bad_lines='skip')\n",
        "data_kandidat.dropna(axis=0, how='all', inplace=True)\n",
        "print(data_kandidat.shape)\n",
        "data_kandidat.head()"
      ],
      "metadata": {
        "id": "fBeRJ8pt3P9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seleksi data"
      ],
      "metadata": {
        "id": "kcLc7oZxb89i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_kandidat.drop_duplicates(subset=['username'], keep='last')\n",
        "data = data.drop_duplicates(keep='last')\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "GaLD58OE3jbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing tanggal"
      ],
      "metadata": {
        "id": "UumCbGgycDTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tanggal = [x for x in data_kandidat['date']]\n",
        "tanggal_general = []\n",
        "for date in tanggal:\n",
        "    new_date = datePreproc(date)\n",
        "    tanggal_general.append(str(new_date))\n",
        "\n",
        "data_kandidat['date'] = tanggal_general"
      ],
      "metadata": {
        "id": "d09ED3KC3gQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing tweet"
      ],
      "metadata": {
        "id": "kI7deZi8cIgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = [x for x in data['content']]\n",
        "tweet_clean = []\n",
        "for i, x in enumerate(tweets):\n",
        "    x_new = preprocess(x)\n",
        "    print('loading for stemming tweet ke-{} dari {}'.format(i+1, len(data['content'])))\n",
        "    tweet_clean.append(str(x_new))"
      ],
      "metadata": {
        "id": "jzoBV99g3p2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "V2iIZLLo3wA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Membuat dataframe tweet yang telah siap dianalisis"
      ],
      "metadata": {
        "id": "GPttEYNBcM3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "def parser_date(date):\n",
        "  new_date = datetime.strptime(date, \"%Y-%m-%d\")\n",
        "  return new_date\n",
        "\n",
        "\n",
        "data_clean = pd.DataFrame(tanggal_general, columns=['date'])\n",
        "data_clean['url'] = data['url']\n",
        "data_clean['username'] = data['username']\n",
        "data_clean['content'] = data['content']\n",
        "data_clean['cleaned'] = tweet_clean\n",
        "data_clean['date'] = data_clean['date'].apply(parser_date)\n",
        "data_clean"
      ],
      "metadata": {
        "id": "SyFJRAJA3xP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_clean.dropna(subset=['cleaned'], inplace=True)\n",
        "print(data_clean.duplicated().any()) \n",
        "print(data_clean.isnull().any() )\n",
        "print(data_clean.head())\n",
        "print(data_clean.shape)"
      ],
      "metadata": {
        "id": "9o8rjVhA33-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Menyimpan file"
      ],
      "metadata": {
        "id": "0EnQyN4BcUjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_clean.to_csv('tweet_clean_prabowo.csv', encoding='utf8', index=False)"
      ],
      "metadata": {
        "id": "TKVUbQzs33t2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}